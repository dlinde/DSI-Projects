{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping and Classifying Indeed Job Postings for Data Occupations\n",
    "\n",
    "### Dimitri Linde / June 2017\n",
    "\n",
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-7d724d66f0524d63c71283eafa60b5e3\" style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Scraping HTML From Relevant Indeed Postings\n",
    "\n",
    "In order to test whether I can a) classify whether a job posting is a data science job or a job in some related field and b) distinguish high-wage jobs from lower wage jobs, I need to scrape job postings first. Below, I've written a function to, given a seed url, crawl the job aggregator www.indeed.com and parse the HTML on each page of postings to extract the urls that contain 'company,' a proxy that a posting is hosted at Indeed. All url's containing 'company are hosted on indeed but not all postings hosted on Indeed contain 'company.' Information on postings hosted on Indeed can, like searching for urls on Indeed, be parsed with a one-size fits all approach. I then append the Indeed postings containing 'company' to a list and, after removing the duplicate values, return the list once the 100 or fewer pages pertaining to the seed url have been crawled. There is also a print function, to let us know the crawl_indeed function has concluded as well as how many postings have been collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get posting urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl_indeed(link):\n",
    "    i = 0\n",
    "    indeed_postings = []\n",
    "    while i < 1000:\n",
    "        response = requests.get(link+str(i))\n",
    "        i +=10\n",
    "        HTML = response.text\n",
    "        tree = html.fromstring(HTML)\n",
    "        urls = tree.xpath('////div[@id]/h2[@id]/a[@class]/@href')\n",
    "        for url in urls:\n",
    "            if 'company' in url:\n",
    "                indeed_url = 'http://www.indeed.com'+url\n",
    "                indeed_postings.append(indeed_url)\n",
    "    indeed_postings = np.unique(indeed_postings)\n",
    "    print len(indeed_postings)\n",
    "    return indeed_postings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50,000-75,000 Data Science, Data Analyst, Analytics, and Business Intelligence Jobs in Boston\n",
    "\n",
    "To get a mix of data science and related occupations, I use the following keywords: \n",
    "* 'data science' \n",
    "* 'data analyst' \n",
    "* 'analytics' \n",
    "* 'business intelligence' \n",
    "\n",
    "I also define three salary tiers:\n",
    "* 50,000 - 75,000\n",
    "* 85,000 - 110,000\n",
    "* 120,000+ (later revised down to 115K+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "66\n",
      "84\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "ds_50 = crawl_indeed(\"https://www.indeed.com/jobs?q=data+science+%2450000-%2475000&l=Boston%2C+MA&start=\")\n",
    "da_50 = crawl_indeed(\"https://www.indeed.com/jobs?q=data+analyst+%2450000-%2475000&l=Boston%2C+MA&start=\")\n",
    "analytics_50 = crawl_indeed(\"https://www.indeed.com/jobs?q=analytics+%2450000-%2475000&l=Boston%2C+MA&start=\")\n",
    "bi_50 = crawl_indeed(\"https://www.indeed.com/jobs?q=business+intelligence+%2450000-%2475000&l=Boston%2C+MA&start=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 85,000 -110,00 Data Science, Data Analyst, Analytics, and Business Intelligence Jobs in Boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "30\n",
      "58\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "ds_85 = crawl_indeed(\"https://www.indeed.com/jobs?q=data+science+%2485000-%24110000&l=Boston%2C+MA&start=\")\n",
    "da_85 = crawl_indeed(\"https://www.indeed.com/jobs?q=data+analyst+%2485000-%24110000&l=Boston%2C+MA&start=\")\n",
    "analytics_85 = crawl_indeed(\"https://www.indeed.com/jobs?q=analytics+%2485000-%24110000&l=Boston%2C+MA&start=\")\n",
    "bi_85 = crawl_indeed(\"https://www.indeed.com/jobs?q=business+intelligence+%2485000-%24110000&l=Boston%2C+MA&start=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 120,000+ Data Science, Data Analyst, Analytics, and Business Intelligence Jobs in Boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "18\n",
      "48\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "ds_120 = crawl_indeed(\"https://www.indeed.com/jobs?q=data+science+%24120000&l=Boston%2C+MA&start=\")\n",
    "da_120 = crawl_indeed(\"https://www.indeed.com/jobs?q=data+analyst+%24120000&l=Boston%2C+MA&start=\")\n",
    "analytics_120 = crawl_indeed(\"https://www.indeed.com/jobs?q=analytics+%24120000&l=Boston%2C+MA&start=\")\n",
    "bi_120 = crawl_indeed(\"https://www.indeed.com/jobs?q=business+intelligence+%24120000&l=Boston%2C+MA&start=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scientist Jobs in San Francisco\n",
    "\n",
    "It will turn out that, after determining which postings I'd consider data science jobs, very few of the postings acquired with the keywords above qualify. The \"data scientist\" keyword turns out to be a far better proxy for data science job postings than \"data science\", so I search for data scientist at the higher income tiers, where we have fewer postings. San Francisco and New York do not churn up enough postings to analyze alone, so I also search across the US. Duplicates will be removed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "dssf_115 = crawl_indeed(\"https://www.indeed.com/jobs?q=data+scientist+%24115%2C000&l=San+Francisco%2C+CA&start=\")\n",
    "dssf_80 = crawl_indeed(\"https://www.indeed.com/jobs?q=data+scientist+%2485000+-+%24110000&l=San+Francisco%2C+CA&start=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scientist Jobs in New York City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "dsny_115 = crawl_indeed(\"https://www.indeed.com/jobs?q=data+scientist+%24115000&l=New+York%2C+NY&start=\")\n",
    "dsny_80 = crawl_indeed(\"https://www.indeed.com/jobs?q=data+scientist+%2485000+-+%24110000&l=New+York%2C+NY&start=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scientist Jobs in the USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "dsusa_115 = crawl_indeed(\"https://www.indeed.com/jobs?q=data+scientist+%24115000&l=United+States&start=\")\n",
    "dsusa_80 = crawl_indeed(\"https://www.indeed.com/jobs?q=data+scientist+%2485000+-+%24110000&l=United+States&start=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate the arrays at the same income level and remove any duplicates\n",
    "\n",
    "This yields one array of postings at each income tier. Duplicates, between, for example, postings in the data science list and postings in the data scientist list, are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n"
     ]
    }
   ],
   "source": [
    "data_50 = np.concatenate((ds_50,da_50,analytics_50,bi_50))\n",
    "data_50 = np.unique(data_50)\n",
    "print len(data_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n"
     ]
    }
   ],
   "source": [
    "data_85 = np.concatenate((ds_85,da_85,analytics_85, bi_85, dssf_80,dsny_80,dsusa_80))\n",
    "data_85 = np.unique(data_85)\n",
    "print len(data_85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n"
     ]
    }
   ],
   "source": [
    "data_120 = np.concatenate((ds_120,da_120,analytics_120,bi_120, dssf_115,dsny_115,dsusa_115))\n",
    "data_120 = np.unique(data_120)\n",
    "print len(data_120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe\n",
    "\n",
    "With the url's for relevant job postings that I've collected, I first want to put them into a dataframe. Once there, I want to iterate through the urls, extracting the HTML from that page and inserting it into the dataframe. Later, I'll use the HTML to extract information like job requirements and title, but which, once saved in a dataframe, I can access at any time. I'll also tag the postings with their income level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indeed_df(array, income_tier):\n",
    "    df = pd.DataFrame(array, columns=['url'])\n",
    "    for i in df.index:\n",
    "        response = requests.get(df['url'][i])\n",
    "        df.ix[i,'HTML'] = response.text\n",
    "    df['category'] = income_tier\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>HTML</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.indeed.com/company/2020/jobs/Market...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>50,000-75,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.indeed.com/company/AA-Search/jobs/M...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>50,000-75,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.indeed.com/company/ABACS/jobs/Behav...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>50,000-75,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.indeed.com/company/AGENCY-451/jobs/...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>50,000-75,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.indeed.com/company/AKRAYA/jobs/Juni...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>50,000-75,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  http://www.indeed.com/company/2020/jobs/Market...   \n",
       "1  http://www.indeed.com/company/AA-Search/jobs/M...   \n",
       "2  http://www.indeed.com/company/ABACS/jobs/Behav...   \n",
       "3  http://www.indeed.com/company/AGENCY-451/jobs/...   \n",
       "4  http://www.indeed.com/company/AKRAYA/jobs/Juni...   \n",
       "\n",
       "                                                HTML       category  \n",
       "0  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  50,000-75,000  \n",
       "1  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  50,000-75,000  \n",
       "2  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  50,000-75,000  \n",
       "3  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  50,000-75,000  \n",
       "4  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  50,000-75,000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_50_df = indeed_df(data_50, '50,000-75,000')\n",
    "data_50_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>HTML</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.indeed.com/company/1988/jobs/Sybase...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>85,000-110,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.indeed.com/company/AA-Search/jobs/F...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>85,000-110,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.indeed.com/company/AFFOA/jobs/Deput...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>85,000-110,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.indeed.com/company/AGENCY-451/jobs/...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>85,000-110,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.indeed.com/company/AKRAYA/jobs/Busi...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>85,000-110,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  http://www.indeed.com/company/1988/jobs/Sybase...   \n",
       "1  http://www.indeed.com/company/AA-Search/jobs/F...   \n",
       "2  http://www.indeed.com/company/AFFOA/jobs/Deput...   \n",
       "3  http://www.indeed.com/company/AGENCY-451/jobs/...   \n",
       "4  http://www.indeed.com/company/AKRAYA/jobs/Busi...   \n",
       "\n",
       "                                                HTML        category  \n",
       "0  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  85,000-110,000  \n",
       "1  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  85,000-110,000  \n",
       "2  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  85,000-110,000  \n",
       "3  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  85,000-110,000  \n",
       "4  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  85,000-110,000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_85_df = indeed_df(data_85, '85,000-110,000')\n",
    "data_85_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>HTML</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.indeed.com/company/%E5%B9%BF%E4%B8%...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>115,000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.indeed.com/company/3sixtyHR/jobs/De...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>115,000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.indeed.com/company/A-Priori-Investm...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>115,000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.indeed.com/company/AA-Search/jobs/E...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>115,000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.indeed.com/company/ACV-Auctions/job...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>115,000+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  http://www.indeed.com/company/%E5%B9%BF%E4%B8%...   \n",
       "1  http://www.indeed.com/company/3sixtyHR/jobs/De...   \n",
       "2  http://www.indeed.com/company/A-Priori-Investm...   \n",
       "3  http://www.indeed.com/company/AA-Search/jobs/E...   \n",
       "4  http://www.indeed.com/company/ACV-Auctions/job...   \n",
       "\n",
       "                                                HTML  category  \n",
       "0  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  115,000+  \n",
       "1  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  115,000+  \n",
       "2  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  115,000+  \n",
       "3  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  115,000+  \n",
       "4  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  115,000+  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_115_df = indeed_df(data_120, '115,000+')\n",
    "data_115_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate the dataframes to get a master df\n",
    "\n",
    "Lastly, I take the dataframes for the separate income tiers and concatenate them into one dataframe. I check for duplicates just in case and remove them if they appear; in this case, they don't. Lastly, we expert this dataframe to a .csv so that we can upload what we need to analyze without having to scrape new postings each time. That took a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641\n",
      "641\n"
     ]
    }
   ],
   "source": [
    "frames = [data_50_df, data_85_df, data_115_df]\n",
    "all_postings = pd.concat(frames, ignore_index=True)\n",
    "print all_postings.shape[0]\n",
    "all_postings = all_postings.drop_duplicates()\n",
    "print all_postings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_postings.to_csv(\"./scraped_postings\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2: Parsing The HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scraped_postings = pd.read_csv(\"./scraped_postings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After uploading our dataframe of job postings, I use a series of path expressions to parse the HTML. Each query works to extract fields of information from job descriptions hosted on Indeed, of which all postings in the dataframe are. Specifically, I extract the job title, company, requirements, and summary fields. The summary field encompasses the information from the three former fields as well as all other information in a posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>HTML</th>\n",
       "      <th>category</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>requirements</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.indeed.com/company/2020/jobs/Market...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>50,000-75,000</td>\n",
       "      <td>[Marketing Manager]</td>\n",
       "      <td>[2020 On-site]</td>\n",
       "      <td>['Run 2020’s marketing strategy and execution'...</td>\n",
       "      <td>['Are you a results-oriented B2B marketing pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  http://www.indeed.com/company/2020/jobs/Market...   \n",
       "\n",
       "                                                HTML       category  \\\n",
       "0  <!DOCTYPE html>\\n<html lang=\"en\">\\n<head><titl...  50,000-75,000   \n",
       "\n",
       "             job_title         company  \\\n",
       "0  [Marketing Manager]  [2020 On-site]   \n",
       "\n",
       "                                        requirements  \\\n",
       "0  ['Run 2020’s marketing strategy and execution'...   \n",
       "\n",
       "                                             summary  \n",
       "0  ['Are you a results-oriented B2B marketing pro...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0,len(scraped_postings)):\n",
    "    HTML = scraped_postings['HTML'][i]\n",
    "    tree = html.fromstring(HTML)\n",
    "    scraped_postings.ix[i,'job_title'] = tree.xpath('//b[@class=\"jobtitle\"]/font/text()')\n",
    "    scraped_postings.ix[i,'company'] = tree.xpath('//td[1]/div/span[@class=\"company\"]/text()')\n",
    "    scraped_postings.ix[i,'requirements'] = str(tree.xpath('//span[@id=\"job_summary\"]/ul/li/text()'))\n",
    "    scraped_postings.ix[i,'summary'] = str(tree.xpath('//span[@id=\"job_summary\"]//text()'))\n",
    "scraped_postings.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of my aim here is to classify data science jobs from related jobs, and job postings don't come tagged as data science jobs or not, I subjectively develop criteria based on scanning the list of job titles in the dataframe. Because there are references to, for example, \"Hadoop\" and \"hadoop,\" I make all text lowercase. About 20% of job titles in the dataframe meet my criteria of being a data science job. The most common job title among these jobs is, not surprisingly, data scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>HTML</th>\n",
       "      <th>category</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>requirements</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.indeed.com/company/2020/jobs/market...</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>50,000-75,000</td>\n",
       "      <td>['marketing manager']</td>\n",
       "      <td>['2020 on-site']</td>\n",
       "      <td>['run 2020’s marketing strategy and execution'...</td>\n",
       "      <td>['are you a results-oriented b2b marketing pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  http://www.indeed.com/company/2020/jobs/market...   \n",
       "\n",
       "                                                HTML       category  \\\n",
       "0  <!doctype html>\\n<html lang=\"en\">\\n<head><titl...  50,000-75,000   \n",
       "\n",
       "               job_title           company  \\\n",
       "0  ['marketing manager']  ['2020 on-site']   \n",
       "\n",
       "                                        requirements  \\\n",
       "0  ['run 2020’s marketing strategy and execution'...   \n",
       "\n",
       "                                             summary  \n",
       "0  ['are you a results-oriented b2b marketing pro...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_postings = scraped_postings.apply(lambda x: x.astype(str).str.lower())\n",
    "scraped_postings.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = (scraped_postings['job_title'].str.contains('data scien'))| (\n",
    "    scraped_postings['job_title'].str.contains('big data')) |  (scraped_postings['job_title'].str.contains('machine')) | (\n",
    "    scraped_postings['job_title'].str.contains('model')) | (\n",
    "    scraped_postings['job_title'].str.contains('business analytics')) | (\n",
    "    scraped_postings['job_title'].str.contains('hadoop'))\n",
    "ds_jobs = scraped_postings[mask].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scraped_postings['ds_job'] = 0\n",
    "for i in ds_jobs:\n",
    "    scraped_postings.ix[i,'ds_job'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>HTML</th>\n",
       "      <th>category</th>\n",
       "      <th>job_title</th>\n",
       "      <th>summary</th>\n",
       "      <th>ds_job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>http://www.indeed.com/company/trovagene,-inc./...</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>115,000+</td>\n",
       "      <td>['scientist/sr. scientist computational biology']</td>\n",
       "      <td>['scientist/senior scientist computational bio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>http://www.indeed.com/company/jd.com/jobs/data...</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"en\"&gt;\\n&lt;head&gt;&lt;titl...</td>\n",
       "      <td>115,000+</td>\n",
       "      <td>['data scientist']</td>\n",
       "      <td>['responsible for the continuous optimization ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url  \\\n",
       "615  http://www.indeed.com/company/trovagene,-inc./...   \n",
       "542  http://www.indeed.com/company/jd.com/jobs/data...   \n",
       "\n",
       "                                                  HTML  category  \\\n",
       "615  <!doctype html>\\n<html lang=\"en\">\\n<head><titl...  115,000+   \n",
       "542  <!doctype html>\\n<html lang=\"en\">\\n<head><titl...  115,000+   \n",
       "\n",
       "                                             job_title  \\\n",
       "615  ['scientist/sr. scientist computational biology']   \n",
       "542                                 ['data scientist']   \n",
       "\n",
       "                                               summary  ds_job  \n",
       "615  ['scientist/senior scientist computational bio...       0  \n",
       "542  ['responsible for the continuous optimization ...       1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = scraped_postings[['url','HTML','category','job_title','summary','ds_job']]\n",
    "subset = subset.sample(n=2)\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "79\n",
      "['data scientist']                        30\n",
      "['senior data scientist']                  5\n",
      "['machine learning engineer']              5\n",
      "['big data engineer']                      2\n",
      "['principal data scientist']               2\n",
      "['lead hadoop developer']                  1\n",
      "['machine learning software engineer']     1\n",
      "['practice leader - big data']             1\n",
      "['big data/ hadoop architect']             1\n",
      "['data architect / data modeler (pf)']     1\n",
      "Name: job_title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mask = scraped_postings['ds_job']==1\n",
    "ds_titles = scraped_postings[mask]['job_title']\n",
    "print(len(ds_titles))\n",
    "print(ds_titles.nunique())\n",
    "print(ds_titles.value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Classifying Job Postings by Income-Level and Occupation-Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, learning_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "For anyone who would like to replicate this: you will need to download the nltk stopwords corpus before being able to use them and you can find simple instructions for how to do so here:\n",
    "http://blog.nlpapi.co/how-to-install-nltk-corporastopwords/. I will expand on the list of stopwords to remove recurring character fragments as well as generic words in the posting summaries. Lastly, I will remove the 'u'' characters commonly preceding words parsed from HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english = stopwords.words('english')\n",
    "english[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_to_stop = ['xe2', 'x80', 'x99s', 'xc2', 'xb7', 'xef', 'x82', 'x99', 'x93', 'x99ll', 'x9d', 'xac', 'x84',\n",
    "               'xa2','xc3']\n",
    "english = english + add_to_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scraped_postings['summary'] = scraped_postings['summary'].str.replace(\"u'\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a Snapshot of The Sample\n",
    "\n",
    "The following function will take in the summary column from the dataframe of parsed postings and return a sorted array of either the most frequent words or the highest term weights, depending on our choice of vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_freq_words(sparse_counts, columns):\n",
    "    # X_all is a sparse matrix, so sum() returns a 'matrix' datatype ...\n",
    "    #   which we then convert into a 1-D ndarray for sorting\n",
    "    word_counts = np.asarray(X_all.sum(axis=0)).reshape(-1)\n",
    "\n",
    "    # argsort() returns smallest first, so we reverse the result\n",
    "    largest_count_indices = word_counts.argsort()[::-1]\n",
    "    freq_words = pd.Series(word_counts[largest_count_indices], \n",
    "                           index=columns[largest_count_indices])\n",
    "\n",
    "    return freq_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of most commonly occuring words across the 600+ job postings in the dataframe are largely generic though the frequency of \"data\" offers a hint that these postings differ from general business opporunities. The same is true of the most common n-grams, excepting, of course, machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data           2467\n",
       "skills         1218\n",
       "development    1063\n",
       "team           1013\n",
       "management      867\n",
       "ability         854\n",
       "time            814\n",
       "strong          736\n",
       "analysis        714\n",
       "support         705\n",
       "dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#most common words, all postings\n",
    "cvt      =  CountVectorizer(strip_accents='unicode', ngram_range=(1,1), stop_words=english, min_df=5)\n",
    "X_all    =  cvt.fit_transform(scraped_postings['summary'])\n",
    "columns  =  np.array(cvt.get_feature_names()) \n",
    "\n",
    "freq_words = get_freq_words(X_all, columns)\n",
    "freq_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full time               555\n",
       "type full time          504\n",
       "type full               504\n",
       "machine learning        306\n",
       "education bachelor      291\n",
       "communication skills    252\n",
       "computer science        206\n",
       "000 00                  180\n",
       "bachelor degree         179\n",
       "time education          170\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#most common ngrams, all postings\n",
    "cvt = CountVectorizer(stop_words=english, ngram_range=(2,4))\n",
    "X_all = cvt.fit_transform(scraped_postings['summary'])\n",
    "columns  =  np.array(cvt.get_feature_names())\n",
    "\n",
    "freq_words = get_freq_words(X_all, columns)\n",
    "freq_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking across the postings, the term weights don't tell us much more than the counts - the yield is still largely general business in nature. Marketing does make an appearance in this list, notable next to the other terms for being either a specific job function or department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data           38.679487\n",
       "development    20.318091\n",
       "skills         19.813484\n",
       "marketing      17.813866\n",
       "team           17.633620\n",
       "analysis       16.456983\n",
       "management     16.433427\n",
       "ability        15.924655\n",
       "learning       15.812633\n",
       "software       14.761406\n",
       "dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#highest tfidf, all postings\n",
    "tfidf = TfidfVectorizer(stop_words=english)\n",
    "X_all = tfidf.fit_transform(scraped_postings['summary'])\n",
    "columns  =  np.array(tfidf.get_feature_names())\n",
    "\n",
    "freq_words = get_freq_words(X_all, columns)\n",
    "freq_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll preemptively ignore some generic recurring business terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_to_stop = ['experience','business','required','work','years','job','project']\n",
    "english = english + add_to_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Income-Level\n",
    "\n",
    "As we have three income levels, the majority class rate is the baseline. 40% of the postings in the dataframe are in the 80K-115K per year range. If we guessed that each posting was at this income-level, we would be correct 40% of the time. Thus, the goal of our classifier is to beat this number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting categories to labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "scraped_postings['category'] = le.fit_transform(scraped_postings['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39781591263650545"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#income-level baseline\n",
    "mask = scraped_postings['category'] == 2\n",
    "float(len(scraped_postings[mask])) / scraped_postings.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conducting 3-fold cross-validation, we first notice substantial spread in the results of the Logistic Regression, likely indicative of having more than an ideal number of features. That being said, even the lowest score yielded by cross-validation is nearly 50% more accurate than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.59069767  0.657277    0.65258216]\n"
     ]
    }
   ],
   "source": [
    "cvt = CountVectorizer(stop_words=english, ngram_range=(1,2))\n",
    "X_all = cvt.fit_transform(scraped_postings['summary'])\n",
    "y = scraped_postings['category']\n",
    "print(cross_val_score(LogisticRegression(), X_all, y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the regression on 80% of the posting data yields accurate predictions in 69% of cases. That's an awfully impressive lift of 1.77 (69/39) relative to the baseline. My sense is that the score on the testing split is higher than in cross validation because the classifier was fit on a larger sample of data. It's worth nothing that this result was acheived purely with default parameters and could very likely be improved through grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = scraped_postings['summary'] \n",
    "y = scraped_postings['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68992248062015504"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvt = CountVectorizer(stop_words=english,  ngram_range=(1,2))\n",
    "pipeline = Pipeline([\n",
    "    ('vect', cvt),\n",
    "#    ('tfidf', TfidfTransformer()),\n",
    "    ('cls', LogisticRegression())\n",
    "]) \n",
    "pipeline.fit(X_train, y_train)\n",
    "predicted = pipeline.predict(X_test)\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report shows that the classifier worked fairly similarly with each income-class. Precision at 2/3 or better demonstrates that classifications were 2/3 or more likely to be correct. There is more disparity in recall: the majority of the total lowest income-tier postings were classified as such but that number is just under 2/3 for the other income classes. At a high level, the classifier did the best job classifying the lowest income-jobs and, relatively speaking, did the worst classifying the middle-tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       120+       0.73      0.65      0.69        34\n",
      "        <75       0.67      0.82      0.74        39\n",
      "     80-115       0.69      0.62      0.65        56\n",
      "\n",
      "avg / total       0.69      0.69      0.69       129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predicted, target_names=[ \"120+\", \"<75\", \"80-115\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Predictors - Income\n",
    "\n",
    "Logistic regression provides additional insight about what differentiates the highest income jobs in the dataset via it’s coefficient output. Though data science jobs represent less than 20% of the collected postings, they are disproportionately concentrated in the high salary tiers, and the best predictors of high salary positions are, in order: “machine learning,” “big data,” and “data science.” The feature occurring in multiple forms negatively predicting high-wage work in the sample: a bachelor’s degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = cvt.fit_transform(scraped_postings['summary'])\n",
    "y = scraped_postings['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "#lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>machine learning</th>\n",
       "      <td>0.397822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big data</th>\n",
       "      <td>0.224195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data science</th>\n",
       "      <td>0.212678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep learning</th>\n",
       "      <td>0.205199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 00</th>\n",
       "      <td>0.175656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing cloud</th>\n",
       "      <td>0.121114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000 00 year</th>\n",
       "      <td>0.117096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data mining</th>\n",
       "      <td>0.113404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data scientists</th>\n",
       "      <td>0.113225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      coef\n",
       "machine learning  0.397822\n",
       "big data          0.224195\n",
       "data science      0.212678\n",
       "deep learning     0.205199\n",
       "000 00            0.175656\n",
       "marketing cloud   0.121114\n",
       "000 00 year       0.117096\n",
       "data mining       0.113404\n",
       "data scientists   0.113225"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array(cvt.get_feature_names())\n",
    "lr_coefs = pd.DataFrame({'coef':lr.coef_[0]},index=features)\n",
    "lr_coefs = lr_coefs.sort_values('coef',ascending=False)\n",
    "lr_coefs.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>type full</th>\n",
       "      <td>-0.261128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type full time</th>\n",
       "      <td>-0.261128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bachelor degree</th>\n",
       "      <td>-0.292407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full time</th>\n",
       "      <td>-0.329437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education bachelor</th>\n",
       "      <td>-0.344268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        coef\n",
       "type full          -0.261128\n",
       "type full time     -0.261128\n",
       "bachelor degree    -0.292407\n",
       "full time          -0.329437\n",
       "education bachelor -0.344268"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_coefs.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characterizing Job Postings by Income-Tier\n",
    "\n",
    "Though the word counts were better predictors for the classifier than the term weights, I think the term weights offer more intuitive insight. Looking at the postings by income-tier reveals that \"marketing\" has the highest term-weight for the < 75K per year postings. \"Research\" and \"support\" also stick out for this tier, the latter term perhaps contrasting with the salience of \"development\" and \"design\" in higher income tiers. The middle-income tier uniquely prizes \"analysis\" and, like the highest tier, has a larger weight for data than the sub-75 tier. The highest income-tier seems to be disproportionately populated by data science roles, yielding \"machine learning\" and \"analytics\" in the list of top 10 term weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marketing     4.917930\n",
       "data          3.265247\n",
       "skills        2.947199\n",
       "sales         2.692812\n",
       "ability       2.606512\n",
       "management    2.575037\n",
       "clinical      2.549105\n",
       "team          2.505177\n",
       "support       2.484247\n",
       "research      2.190723\n",
       "dtype: float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = scraped_postings['category'] == 1 ##1 corresponds to < 75K per year\n",
    "tfidf = TfidfVectorizer(stop_words=english, ngram_range=(1,4))\n",
    "X_all = tfidf.fit_transform(scraped_postings.ix[mask,'summary'])\n",
    "columns  =  np.array(tfidf.get_feature_names())\n",
    "\n",
    "freq_words = get_freq_words(X_all, columns)\n",
    "freq_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data           6.683377\n",
       "development    4.128827\n",
       "skills         3.524531\n",
       "analysis       3.149571\n",
       "team           3.052998\n",
       "software       3.045155\n",
       "clinical       3.021502\n",
       "ability        2.778972\n",
       "systems        2.700767\n",
       "strong         2.667731\n",
       "dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = scraped_postings['category'] == 2\n",
    "tfidf = TfidfVectorizer(stop_words=english, ngram_range=(1,4))\n",
    "X_all = tfidf.fit_transform(scraped_postings.ix[mask,'summary'])\n",
    "columns  =  np.array(tfidf.get_feature_names())\n",
    "\n",
    "freq_words = get_freq_words(X_all, columns)\n",
    "freq_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data                6.820855\n",
       "learning            3.341324\n",
       "development         2.921638\n",
       "machine             2.541522\n",
       "machine learning    2.526492\n",
       "team                2.439990\n",
       "management          2.407016\n",
       "analytics           2.374114\n",
       "skills              2.366834\n",
       "design              2.218176\n",
       "dtype: float64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = scraped_postings['category'] == 0\n",
    "tfidf = TfidfVectorizer(stop_words=english, ngram_range=(1,4))\n",
    "X_all = tfidf.fit_transform(scraped_postings.ix[mask,'summary'])\n",
    "columns  =  np.array(tfidf.get_feature_names())\n",
    "\n",
    "freq_words = get_freq_words(X_all, columns)\n",
    "freq_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAFJCAYAAACimpYTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wBi7BW1+DODKwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x130f7b5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fw = freq_words[:10]\n",
    "fw.plot(kind='barh')\n",
    "#sns.countplot(x=lr.index, data=lr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Data Science Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second prompt, classifying data science jobs, boosted decision trees turn out to be a better classifier than logisitic regression. Whereas the latter classifier roughly achieves the baseline of 82% accuracy - here meaning a guess that each posting in our simple does not represent a data science job - we generate a lift of 1.13 using boosted decision trees with default parameters and simple word count. As above, the abundance of features and small cross validation sizes yield significant spread in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8159126365054602"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ds_jobs baseline\n",
    "mask = scraped_postings['ds_job'] == 1\n",
    "1- (float(len(scraped_postings[mask])) / scraped_postings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.93023256  0.89671362  0.85915493]\n"
     ]
    }
   ],
   "source": [
    "X_all = cvt.fit_transform(scraped_postings['summary'])\n",
    "y = scraped_postings['ds_job']\n",
    "print(cross_val_score(AdaBoostClassifier(), X_all, y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = scraped_postings['summary']\n",
    "y = scraped_postings['ds_job']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92248062015503873"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvt = CountVectorizer(stop_words=english,  ngram_range=(1,2))\n",
    "pipeline = Pipeline([\n",
    "    ('vect', cvt),\n",
    "#    ('tfidf', TfidfTransformer()),\n",
    "    ('cls', AdaBoostClassifier())\n",
    "]) \n",
    "pipeline.fit(X_train, y_train)\n",
    "predicted = pipeline.predict(X_test)\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report demonstrates that the classifier precisely tagged jobs that were not data science jobs - in 95% of cases, jobs tagged as not data science jobs were not data science jobs. Likewise, the classifier tagged a higher rate of total non-data science postings, nearly 96%. That said, the classifier performed very well predicting the minority class of data science jobs, guessing correctly 78% of the time and identifying 70% of cases. These results seem particularly impressive when reconsidering that I tagged the postings as data science jobs or not subjectively, based on a quick scan of job titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      other       0.95      0.96      0.95       109\n",
      "     ds_job       0.78      0.70      0.74        20\n",
      "\n",
      "avg / total       0.92      0.92      0.92       129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predicted, target_names=[\"other\", \"ds_job\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Predictors - Occupation\n",
    "\n",
    "Though boosted decision trees provided a better score, logistic regression coefficients derived from word counts once again provide insight about what differentiates the occupation groups. The strongest predictors of whether a job is a data science job are identical to those terms likewise predicting high salary. We also see the presence of data mining, predictive modeling, data visualization, and deep learning, among other terms, as predicting data science roles. Negatively predicting a data science role, as with income: requiring only a bachelor’s degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = cvt.fit_transform(scraped_postings['summary'])\n",
    "y = scraped_postings['ds_job']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>machine learning</th>\n",
       "      <td>0.722316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data scientist</th>\n",
       "      <td>0.432207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big data</th>\n",
       "      <td>0.419141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data science</th>\n",
       "      <td>0.330205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictive models</th>\n",
       "      <td>0.134650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data visualization</th>\n",
       "      <td>0.118815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictive modeling</th>\n",
       "      <td>0.104879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep learning</th>\n",
       "      <td>0.104829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning techniques</th>\n",
       "      <td>0.103479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data mining</th>\n",
       "      <td>0.100352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real time</th>\n",
       "      <td>0.091732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine learning techniques</th>\n",
       "      <td>0.091576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insurance marketing</th>\n",
       "      <td>0.075128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantitative discipline</th>\n",
       "      <td>0.073246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algorithm development</th>\n",
       "      <td>0.072297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 coef\n",
       "machine learning             0.722316\n",
       "data scientist               0.432207\n",
       "big data                     0.419141\n",
       "data science                 0.330205\n",
       "predictive models            0.134650\n",
       "data visualization           0.118815\n",
       "predictive modeling          0.104879\n",
       "deep learning                0.104829\n",
       "learning techniques          0.103479\n",
       "data mining                  0.100352\n",
       "real time                    0.091732\n",
       "machine learning techniques  0.091576\n",
       "insurance marketing          0.075128\n",
       "quantitative discipline      0.073246\n",
       "algorithm development        0.072297"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array(cvt.get_feature_names())\n",
    "lr_coefs = pd.DataFrame({'coef':lr.coef_[0]},index=features)\n",
    "lr_coefs = lr_coefs.sort_values('coef',ascending=False)\n",
    "lr_coefs.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>communication skills</th>\n",
       "      <td>-0.225888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education bachelor</th>\n",
       "      <td>-0.354551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type full time</th>\n",
       "      <td>-0.410733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type full</th>\n",
       "      <td>-0.410733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full time</th>\n",
       "      <td>-0.454437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          coef\n",
       "communication skills -0.225888\n",
       "education bachelor   -0.354551\n",
       "type full time       -0.410733\n",
       "type full            -0.410733\n",
       "full time            -0.454437"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_coefs.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characterizing Job Postings by Occupation\n",
    "\n",
    "As above, I use term frequency to characterize data science jobs and data related jobs though the classifier performed better using word counts. The distinctions here are much clearer than that of income level. Data related (non Data Science) jobs in the sample commonly invoke marketing, clinical, and research applications. Software is likewise common but no specific tools or applications are listed. Data Science jobs, on the other hand, commonly invoke machine learning, analytics, modeling, python, and big data. I think the data science jobs, via these terms, offer a much stronger sense of coherence than the broader group of data related jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data           9.189892\n",
       "development    6.979458\n",
       "marketing      6.885673\n",
       "skills         6.433163\n",
       "team           5.898329\n",
       "sales          5.884908\n",
       "clinical       5.803983\n",
       "management     5.725871\n",
       "ability        5.495082\n",
       "support        5.203179\n",
       "research       5.033285\n",
       "analysis       5.020574\n",
       "software       4.952255\n",
       "strong         4.815434\n",
       "systems        4.758234\n",
       "dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf for postings not tagged data science postings\n",
    "mask = scraped_postings['ds_job'] == 0\n",
    "tfidf = TfidfVectorizer(stop_words=english, ngram_range=(1,4))\n",
    "X_all = tfidf.fit_transform(scraped_postings.ix[mask,'summary'])\n",
    "columns  =  np.array(tfidf.get_feature_names())\n",
    "\n",
    "freq_words = get_freq_words(X_all, columns)\n",
    "freq_words[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data                6.915183\n",
       "learning            3.029908\n",
       "machine             2.414281\n",
       "machine learning    2.390501\n",
       "science             1.982925\n",
       "skills              1.788962\n",
       "analysis            1.770923\n",
       "analytics           1.761157\n",
       "models              1.669913\n",
       "big                 1.576679\n",
       "python              1.516496\n",
       "data science        1.508542\n",
       "development         1.503688\n",
       "big data            1.501329\n",
       "statistical         1.455048\n",
       "dtype: float64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf for postings not tagged data science postings\n",
    "mask = scraped_postings['ds_job'] == 1\n",
    "tfidf = TfidfVectorizer(stop_words=english, ngram_range=(1,4))\n",
    "X_all = tfidf.fit_transform(scraped_postings.ix[mask,'summary'])\n",
    "columns  =  np.array(tfidf.get_feature_names())\n",
    "\n",
    "freq_words = get_freq_words(X_all, columns)\n",
    "freq_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all. Thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
